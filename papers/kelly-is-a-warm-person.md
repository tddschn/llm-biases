# “Kelly is a Warm Person, Joseph is a Role Model”: Gender Biases in LLM-Generated Reference Letters

![](https://github.com/cli/cli/assets/45612704/d5cdfbbf-6cdf-4ac9-9778-0c43fa4f2e23)

## Code

https://github.com/uclanlp/biases-llm-reference-letters

https://sourcegraph.com/github.com/uclanlp/biases-llm-reference-letters

prompt generation: https://sourcegraph.com/github.com/uclanlp/biases-llm-reference-letters/-/blob/generate_clg.py?L24=

generation: https://github.com/uclanlp/biases-llm-reference-letters/blob/main/generated_letters/chatgpt/clg/clg_letters.csv

- [“Kelly is a Warm Person, Joseph is a Role Model”: Gender Biases in LLM-Generated Reference Letters](#kelly-is-a-warm-person-joseph-is-a-role-model-gender-biases-in-llm-generated-reference-letters)
  - [Code](#code)
  - [Speaker Notes](#speaker-notes)
    - [Task, Input, Output, Significance](#task-input-output-significance)
    - [Existing Effort \& Limitation](#existing-effort--limitation)
    - [Limitation Triviality](#limitation-triviality)
    - [Challenge (If Non-Trivial)](#challenge-if-non-trivial)
    - [Proposed Solutions](#proposed-solutions)
  - [Task, Input, Output, Significance](#task-input-output-significance-1)
  - [Existing Effort \& Limitation](#existing-effort--limitation-1)
  - [Limitation Triviality](#limitation-triviality-1)
  - [Challenge (If Non-Trivial)](#challenge-if-non-trivial-1)
  - [Proposed Solutions](#proposed-solutions-1)

<details>
<summary>longer version</summary>



<!-- ### Task, Input, Output, Significance -->
<!-- - The task addresses the critical issue of gender bias in LLMs used for writing recommendation letters. -->
<!-- - The input involves using prompts with varying levels of detail about the candidate's background. -->
<!-- - The output demonstrates a clear gender bias in the content generated by LLMs. -->
<!-- - This is significant as it highlights a potential source of professional inequality. -->
<!---->
<!-- ### Existing Effort & Limitation -->
<!-- - Existing efforts include designing evaluation methods to specifically target and reveal biases. -->
<!-- - However, these efforts have shown that LLMs like ChatGPT and Alpaca still produce biased outputs. -->
<!---->
<!-- ### Limitation Triviality -->
<!-- - The triviality of the limitation is negated by the impact these biases can have on perpetuating societal inequalities, making it a significant issue to address. -->
<!---->
<!-- ### Challenge (If Non-Trivial) -->
<!-- - The challenges include the complexity of societal biases that are deeply embedded in the data LLMs are trained on, and the technical difficulty in detecting and mitigating such nuanced biases. -->
<!---->
<!-- ### Proposed Solutions -->
<!-- - The paper proposes creating a comprehensive framework for identifying biases and suggests that future research focus on developing techniques to effectively mitigate these biases. -->
<!---->

## Speaker Notes

### Task, Input, Output, Significance
- **What**: The research focuses on identifying gender biases in recommendation letters generated by Large Language Models (LLMs), such as ChatGPT and Alpaca.
- **Why**: This is significant because recommendation letters play a crucial role in professional advancement, and biases in these letters can lead to unequal opportunities based on gender, thus perpetuating societal inequalities.

### Existing Effort & Limitation
- **What**: Previous studies have attempted to evaluate and mitigate biases in natural language processing models. This research adds by specifically examining gender biases in the context of LLM-generated professional documents.
- **Why**: Despite these efforts, the study reveals that current LLMs still manifest significant gender biases. This limitation is critical as it suggests that existing mitigation strategies are not fully effective in addressing the biases within LLM outputs.

### Limitation Triviality
- **What**: The question of whether the limitation (i.e., gender bias in LLM outputs) is trivial is addressed.
- **Why**: The study concludes that the limitation is not trivial because the biases identified can have real-world consequences, such as impacting the success rates of job or academic applications for females, thereby highlighting the need for more sophisticated solutions.

### Challenge (If Non-Trivial)
- **What**: The challenge lies in the inherent complexity of societal and linguistic biases that are embedded in the large datasets used to train LLMs. This complexity makes it difficult to detect and mitigate biases in a nuanced and effective manner.
- **Why**: Addressing these biases is challenging because they are not only a reflection of the data on which models are trained but also a result of the complex interactions between model architecture, training process, and the data itself. Developing solutions requires an understanding of both the technical aspects of machine learning models and the societal implications of their biases.

### Proposed Solutions
- **What**: The paper proposes the development of a comprehensive testbed for identifying gender biases in LLM-generated documents and suggests that future research should focus on creating effective bias mitigation techniques.
- **Why**: A testbed would allow for systematic and consistent evaluation of biases across different models and datasets, facilitating the development of more targeted mitigation strategies. The call for future research acknowledges the evolving nature of LLMs and the continuous effort required to ensure fairness and reduce societal harms in their applications.


</details>

## Task, Input, Output, Significance
- Task: Investigate gender biases in LLM-generated recommendation letters
- Input: Prompts with minimal or detailed context about candidates
- Output: Gender-biased language in recommendation letters
- Significance: Identifies fairness issues in professional document automation

## Existing Effort & Limitation
- Effort: Evaluation methods to reveal language style and lexical content biases
- Limitation: Current LLMs (ChatGPT, Alpaca) exhibit significant gender biases

## Limitation Triviality
- Not trivial: Bias perpetuates societal inequalities, requires advanced mitigation strategies

## Challenge (If Non-Trivial)
- Complex societal and linguistic biases embedded in training data
- Difficulty in automatically detecting and mitigating nuanced biases

## Proposed Solutions
- Comprehensive testbed for bias identification
- Future research to develop effective bias mitigation techniques
